# -*- coding: utf-8 -*-
"""agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U052sNoREA41sE3V4rv1YS4vLGPhlXkO
"""

# --- STEP 1: Install All Libraries ---
# Install everything needed for all steps, just in case
!pip install -U transformers peft bitsandbytes accelerate datasets trl graphviz pymupdf -q
print("‚úÖ All libraries installed.")

import torch
import os
import json
import subprocess # This is what we use to run our tools
import re
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import PeftModel
from IPython.display import display, Image, Markdown # To display the final results

# --- STEP 2: PASTE YOUR TOKEN HERE ---
YOUR_TOKEN = "PASTE_YOUR_HUGGING_FACE_TOKEN_HERE"


# --- STEP 3: Define the Agent's "Model" Component ---
# (This is a function that loads and runs your fine-tuned model)

# Define the model paths
model_name = "google/gemma-2b-it"
lora_model_path = "final_note_extractor_model"

# Define the 4-bit config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

print("--- üß† Loading Fine-Tuned Model (The 'Brain')... ---")
# Load the base model
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    token=YOUR_TOKEN
)
# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    token=YOUR_TOKEN
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
# Merge your LoRA adapter
model = PeftModel.from_pretrained(base_model, lora_model_path)
model.eval() # Set to evaluation mode
print("‚úÖ 'Brain' is loaded and ready.")


def run_note_extractor_model(input_text):
    """
    This function takes raw text, runs it through the fine-tuned model,
    and returns the structured JSON output.
    """
    print("üß† 'Brain' is thinking... Generating structured JSON...")

    prompt = f"""<start_of_turn>user
You are an expert academic assistant. Your task is to extract the hierarchical structure from the following lecture notes and format it as a JSON object.

Lecture Notes:
{input_text}

JSON Output:<end_of_turn>
<start_of_turn>model
"""

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    input_token_count = inputs.input_ids.shape[1]

    outputs = model.generate(
        **inputs,
        max_new_tokens=1024,
        eos_token_id=tokenizer.eos_token_id
    )

    # Decode only the newly generated tokens
    generated_token_ids = outputs[0][input_token_count:]
    generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)
    generated_text = generated_text.strip().replace("<end_of_turn>", "")

    # Robust JSON parsing
    try:
        json_start_index = generated_text.find('{')
        json_end_index = generated_text.rfind('}') + 1
        if json_start_index == -1 or json_end_index == 0:
            raise ValueError("No JSON object found in model output.")

        json_string = generated_text[json_start_index:json_end_index]
        parsed_json = json.loads(json_string)
        print("‚úÖ 'Brain' finished. JSON is valid.")
        return parsed_json

    except Exception as e:
        print(f"‚ùå 'Brain' FAILED to produce valid JSON: {e}")
        return None


# --- STEP 4: Define the Agent's "Tools" ---
# (These functions call the Python scripts you built)

def run_pdf_extractor_tool(input_pdf, output_txt):
    """Runs Tool 1: pdf_extractor.py"""
    print(f"üõ†Ô∏è Calling Tool 1 (PDF Extractor) on {input_pdf}...")
    subprocess.run(["python", "pdf_extractor.py", input_pdf, output_txt], check=True)
    print(f"‚úÖ Tool 1 finished. Text saved to {output_txt}")

def run_markdown_generator_tool(input_json_file, output_md):
    """Runs Tool 2: markdown_generator.py"""
    print(f"üõ†Ô∏è Calling Tool 2 (Note Generator) on {input_json_file}...")
    subprocess.run(["python", "markdown_generator.py", input_json_file, output_md], check=True)
    print(f"‚úÖ Tool 2 finished. Notes saved to {output_md}")

def run_mindmap_generator_tool(input_json_file, output_png):
    """Runs Tool 3: mindmap_generator.py"""
    print(f"üõ†Ô∏è Calling Tool 3 (Mind Map Generator) on {input_json_file}...")
    subprocess.run(["python", "mindmap_generator.py", input_json_file, output_png], check=True)
    print(f"‚úÖ Tool 3 finished. Map saved to {output_png}")


# --- STEP 5: The Agent's "Reason, Plan, Execute" Logic ---

# --- Define filenames ---
INPUT_PDF_FILE = "my_lecture.pdf" # Make sure this file is uploaded!
TEMP_TXT_FILE = "temp_raw_text.txt"
TEMP_JSON_FILE = "temp_model_output.json"
FINAL_NOTES_FILE = "FINAL_NOTES.md"
FINAL_MAP_FILE = "FINAL_MIND_MAP.png"


print("\n--- ü§ñ AGENT: STARTING RUN ---")
print(f"--- üéØ GOAL: Create notes and mind map from {INPUT_PDF_FILE} ---")

try:
    # --- PLAN STEP 1: Extract Text ---
    run_pdf_extractor_tool(INPUT_PDF_FILE, TEMP_TXT_FILE)

    # Read the extracted text
    with open(TEMP_TXT_FILE, 'r', encoding='utf-8') as f:
        lecture_text = f.read()
        # We'll just test the first ~2000 chars to keep it fast
        lecture_text_chunk = lecture_text[:2000]

    # --- PLAN STEP 2: Run Fine-Tuned Model ---
    model_json_output = run_note_extractor_model(lecture_text_chunk)

    if model_json_output is None:
        raise Exception("Model failed to produce output. Stopping agent.")

    # Save the model's JSON to a temp file for our tools
    with open(TEMP_JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(model_json_output, f, indent=2)

    # --- PLAN STEP 3: Generate Short Notes ---
    run_markdown_generator_tool(TEMP_JSON_FILE, FINAL_NOTES_FILE)

    # --- PLAN STEP 4: Generate Mind Map ---
    run_mindmap_generator_tool(TEMP_JSON_FILE, FINAL_MAP_FILE)

    print("\n--- ‚úÖ AGENT: EXECUTION COMPLETE! ---")

    # --- Display the final results ---
    print("\n\n--- üìú FINAL SHORT NOTES ---")
    with open(FINAL_NOTES_FILE, 'r') as f:
        display(Markdown(f.read()))

    print("\n\n--- üé® FINAL MIND MAP ---")
    display(Image(FINAL_MAP_FILE))


except Exception as e:
    print(f"\n--- ‚ùå AGENT: EXECUTION FAILED ---")
    print(f"An error occurred: {e}")