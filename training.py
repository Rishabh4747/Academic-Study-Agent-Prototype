# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G0K6YSHcuDpmI7SbLMeTF5z78gNLL5V0
"""

# Train ---

print("--- 1. Re-installing libraries after restart... ---")
!pip install -U transformers peft bitsandbytes accelerate datasets trl
print("✅ Libraries are ready.")


# --- 2. PASTE YOUR TOKEN HERE ---
YOUR_TOKEN = "PASTE_YOUR_HUGGING_FACE_TOKEN_HERE"


# --- 3. Run the Training Code ---
print("--- 3. Starting the training process... ---")

import torch
import os
import json
from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer


# ------------------------------
# Load the Dataset
# ------------------------------
print("Loading dataset.jsonl...")
dataset_path = "dataset.jsonl"
try:
    dataset = load_dataset('json', data_files=dataset_path, split='train')
    print(f"✅ Successfully loaded {len(dataset)} examples.")
except Exception as e:
    print(f"❌ ERROR: Could not load dataset.jsonl.")
    print(f"Details: {e}")
    raise


# ------------------------------
# Define the Model and Tokenizer
# ------------------------------
model_name = "google/gemma-2b-it"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

print(f"Loading base model: {model_name} (This may take a few minutes...)")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    token=YOUR_TOKEN
)
model.config.use_cache = False
print("✅ Base model loaded.")

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    token=YOUR_TOKEN
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"


# ------------------------------
# Configure LoRA
# ------------------------------
print("Applying LoRA configuration...")
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"]
)
model = get_peft_model(model, lora_config)
print("✅ LoRA applied.")
model.print_trainable_parameters()


# ------------------------------
# Format Dataset
# ------------------------------
def format_example(example):
    output_json = json.dumps(example['output'])
    text = f"""<start_of_turn>user
You are an expert academic assistant. Your task is to extract the hierarchical structure from the following lecture notes and format it as a JSON object.

Lecture Notes:
{example['input']}

JSON Output:<end_of_turn>
<start_of_turn>model
{output_json}<end_of_turn>"""
    return {"text": text}

print("Formatting dataset with prompt template...")
formatted_dataset = dataset.map(format_example)


from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForLanguageModeling

print("Setting up the SFTTrainer...")

# Create a data collator for language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Define the text formatting function
def formatting_func(example):
    return example["text"]

# Initialize the trainer — clean version for new TRL API
trainer = SFTTrainer(
    model=model,
    train_dataset=formatted_dataset,
    peft_config=lora_config,
    data_collator=data_collator,
    args=TrainingArguments(
        output_dir="outputs",
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        num_train_epochs=3,
        learning_rate=2e-4,
        logging_steps=5,
        optim="paged_adamw_8bit",
        save_strategy="epoch",
        fp16=True,
    ),
    formatting_func=formatting_func,
)




# ------------------------------
# Start Fine-Tuning
# ------------------------------
print("\n--- STARTING FINE-TUNING ---")
import os
os.environ["WANDB_DISABLED"] = "true"
trainer.train()

# ------------------------------
# Save Final Model
# ------------------------------
final_model_path = "final_note_extractor_model"
print(f"\n--- Training complete. Saving final model to '{final_model_path}' ---")
trainer.save_model(final_model_path)
print(f"✅ All done. Your fine-tuned model is saved!")